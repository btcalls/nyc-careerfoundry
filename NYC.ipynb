{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics for Beginners\n",
    "## Overview\n",
    "A Data Analytics project based on a [CareerFoundry](https://careerfoundry.com/en/tutorials/data-analytics-for-beginners/introduction-to-data-analytics) tutorial. This project aims to follow the tutorial step-by-step, but using a combination of tech stack such as Python, SQL, and Tableau to practice different key skills at different stages of Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wL1R_Wfqs792",
    "outputId": "7572a187-008c-4ef3-da3d-01b8a9a2f15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in ./.venv/lib/python3.9/site-packages (9.1.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: pymysql in ./.venv/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: sqlalchemy in ./.venv/lib/python3.9/site-packages (2.0.36)\n",
      "Requirement already satisfied: sqlalchemy-utils in ./.venv/lib/python3.9/site-packages (0.41.2)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyDrive2 in ./.venv/lib/python3.9/site-packages (1.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./.venv/lib/python3.9/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.9/site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.5 in ./.venv/lib/python3.9/site-packages (from PyDrive2) (2.154.0)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in ./.venv/lib/python3.9/site-packages (from PyDrive2) (4.1.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in ./.venv/lib/python3.9/site-packages (from PyDrive2) (6.0.2)\n",
      "Requirement already satisfied: cryptography<44 in ./.venv/lib/python3.9/site-packages (from PyDrive2) (43.0.3)\n",
      "Requirement already satisfied: pyOpenSSL<=24.2.1,>=19.1.0 in ./.venv/lib/python3.9/site-packages (from PyDrive2) (24.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.9/site-packages (from cryptography<44->PyDrive2) (1.17.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in ./.venv/lib/python3.9/site-packages (from google-api-python-client>=1.12.5->PyDrive2) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in ./.venv/lib/python3.9/site-packages (from google-api-python-client>=1.12.5->PyDrive2) (2.36.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.venv/lib/python3.9/site-packages (from google-api-python-client>=1.12.5->PyDrive2) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in ./.venv/lib/python3.9/site-packages (from google-api-python-client>=1.12.5->PyDrive2) (2.23.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.venv/lib/python3.9/site-packages (from google-api-python-client>=1.12.5->PyDrive2) (4.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in ./.venv/lib/python3.9/site-packages (from oauth2client>=4.0.0->PyDrive2) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in ./.venv/lib/python3.9/site-packages (from oauth2client>=4.0.0->PyDrive2) (0.4.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in ./.venv/lib/python3.9/site-packages (from oauth2client>=4.0.0->PyDrive2) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in ./.venv/lib/python3.9/site-packages (from oauth2client>=4.0.0->PyDrive2) (1.17.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.9/site-packages (from cffi>=1.12->cryptography<44->PyDrive2) (2.22)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.venv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in ./.venv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (5.29.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.venv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./.venv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.9/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.12.5->PyDrive2) (5.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./.venv/lib/python3.9/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.12.5->PyDrive2) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.5->PyDrive2) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python pandas numpy openpyxl pymysql sqlalchemy sqlalchemy-utils python-dotenv PyDrive2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "\n",
    "def connect_to_drive():\n",
    "  \"\"\"\n",
    "  Initialises connection to Google Drive using PyDrive2. Requires `client_secrets.json`\n",
    "  from your OAuth 2.0 application downloaded and available in the same directory.\n",
    "  \n",
    "  See https://developers.google.com/drive/api/quickstart/python for more information.\n",
    "  \"\"\"\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.LocalWebserverAuth()\n",
    "\n",
    "  return GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "f_tI2B55plr5",
    "outputId": "4ae7674d-5eb3-44c2-9852-598d3728d9e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Stop Time</th>\n",
       "      <th>Start Station ID</th>\n",
       "      <th>Start Station Name</th>\n",
       "      <th>End Station ID</th>\n",
       "      <th>End Station Name</th>\n",
       "      <th>Bike ID</th>\n",
       "      <th>User Type</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Groups</th>\n",
       "      <th>Trip Duration</th>\n",
       "      <th>Trip_Duration_in_min</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:38:00</td>\n",
       "      <td>2017-01-01 01:03:00</td>\n",
       "      <td>3194</td>\n",
       "      <td>McGinley Square</td>\n",
       "      <td>3271</td>\n",
       "      <td>Danforth Light Rail</td>\n",
       "      <td>24668</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1961</td>\n",
       "      <td>60</td>\n",
       "      <td>55-64</td>\n",
       "      <td>1513</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 01:47:00</td>\n",
       "      <td>2017-01-01 01:58:00</td>\n",
       "      <td>3183</td>\n",
       "      <td>Exchange Place</td>\n",
       "      <td>3203</td>\n",
       "      <td>Hamilton Park</td>\n",
       "      <td>26167</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1993</td>\n",
       "      <td>28</td>\n",
       "      <td>25-34</td>\n",
       "      <td>639</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 01:47:00</td>\n",
       "      <td>2017-01-01 01:58:00</td>\n",
       "      <td>3183</td>\n",
       "      <td>Exchange Place</td>\n",
       "      <td>3203</td>\n",
       "      <td>Hamilton Park</td>\n",
       "      <td>26167</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1993</td>\n",
       "      <td>28</td>\n",
       "      <td>25-34</td>\n",
       "      <td>639</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 01:56:00</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>3186</td>\n",
       "      <td>Grove St PATH</td>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>24604</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1970</td>\n",
       "      <td>51</td>\n",
       "      <td>45-54</td>\n",
       "      <td>258</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 02:12:00</td>\n",
       "      <td>2017-01-01 02:23:00</td>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>3206</td>\n",
       "      <td>Hilltop</td>\n",
       "      <td>24641</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1978</td>\n",
       "      <td>43</td>\n",
       "      <td>35-44</td>\n",
       "      <td>663</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Start Time           Stop Time  Start Station ID  \\\n",
       "0 2017-01-01 00:38:00 2017-01-01 01:03:00              3194   \n",
       "1 2017-01-01 01:47:00 2017-01-01 01:58:00              3183   \n",
       "2 2017-01-01 01:47:00 2017-01-01 01:58:00              3183   \n",
       "3 2017-01-01 01:56:00 2017-01-01 02:00:00              3186   \n",
       "4 2017-01-01 02:12:00 2017-01-01 02:23:00              3270   \n",
       "\n",
       "  Start Station Name  End Station ID     End Station Name  Bike ID  \\\n",
       "0    McGinley Square            3271  Danforth Light Rail    24668   \n",
       "1     Exchange Place            3203        Hamilton Park    26167   \n",
       "2     Exchange Place            3203        Hamilton Park    26167   \n",
       "3      Grove St PATH            3270      Jersey & 6th St    24604   \n",
       "4    Jersey & 6th St            3206              Hilltop    24641   \n",
       "\n",
       "    User Type  Birth Year  Age Age Groups  Trip Duration  \\\n",
       "0  Subscriber        1961   60      55-64           1513   \n",
       "1  Subscriber        1993   28      25-34            639   \n",
       "2  Subscriber        1993   28      25-34            639   \n",
       "3  Subscriber        1970   51      45-54            258   \n",
       "4  Subscriber        1978   43      35-44            663   \n",
       "\n",
       "   Trip_Duration_in_min  Month  Season  Temperature Weekday  \n",
       "0                    25      1  Winter           10  Sunday  \n",
       "1                    11      1  Winter           10  Sunday  \n",
       "2                    11      1  Winter           10  Sunday  \n",
       "3                     4      1  Winter           10  Sunday  \n",
       "4                    11      1  Winter           10  Sunday  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read .xlsx file locally\n",
    "df = pd.read_excel(\"nyc_bikes.xlsx\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N6R_rNytj_Tr",
    "outputId": "5f887e52-a6ce-499c-97ac-d0f644685e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20400 entries, 0 to 20399\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   Start Time            20400 non-null  datetime64[ns]\n",
      " 1   Stop Time             20400 non-null  datetime64[ns]\n",
      " 2   Start Station ID      20400 non-null  int64         \n",
      " 3   Start Station Name    20400 non-null  object        \n",
      " 4   End Station ID        20400 non-null  int64         \n",
      " 5   End Station Name      20399 non-null  object        \n",
      " 6   Bike ID               20400 non-null  int64         \n",
      " 7   User Type             20400 non-null  object        \n",
      " 8   Birth Year            20400 non-null  int64         \n",
      " 9   Age                   20400 non-null  int64         \n",
      " 10  Age Groups            20400 non-null  object        \n",
      " 11  Trip Duration         20400 non-null  int64         \n",
      " 12  Trip_Duration_in_min  20400 non-null  int64         \n",
      " 13  Month                 20400 non-null  int64         \n",
      " 14  Season                20400 non-null  object        \n",
      " 15  Temperature           20400 non-null  int64         \n",
      " 16  Weekday               20400 non-null  object        \n",
      "dtypes: datetime64[ns](2), int64(9), object(6)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display column information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "collapsed": true,
    "id": "5CWiZHmvkJOT",
    "outputId": "a1e0c51b-aede-4adb-e4a2-0bd9d2cdfc3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Stop Time</th>\n",
       "      <th>Start Station ID</th>\n",
       "      <th>End Station ID</th>\n",
       "      <th>Bike ID</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Trip Duration</th>\n",
       "      <th>Trip_Duration_in_min</th>\n",
       "      <th>Month</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20400</td>\n",
       "      <td>20400</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "      <td>20400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2017-02-21 20:03:51.773529344</td>\n",
       "      <td>2017-02-21 20:13:16.335293952</td>\n",
       "      <td>3215.863627</td>\n",
       "      <td>3211.439510</td>\n",
       "      <td>25301.732647</td>\n",
       "      <td>1979.319706</td>\n",
       "      <td>41.680294</td>\n",
       "      <td>563.842745</td>\n",
       "      <td>9.398775</td>\n",
       "      <td>2.221569</td>\n",
       "      <td>14.897647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2017-01-01 00:38:00</td>\n",
       "      <td>2017-01-01 01:03:00</td>\n",
       "      <td>3183.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>15084.000000</td>\n",
       "      <td>1931.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2017-01-31 19:00:15</td>\n",
       "      <td>2017-01-31 19:04:45</td>\n",
       "      <td>3186.000000</td>\n",
       "      <td>3186.000000</td>\n",
       "      <td>24523.000000</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017-02-27 07:20:00</td>\n",
       "      <td>2017-02-27 07:29:30</td>\n",
       "      <td>3203.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>24679.000000</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017-03-11 20:38:00</td>\n",
       "      <td>2017-03-11 20:49:00</td>\n",
       "      <td>3267.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>26220.000000</td>\n",
       "      <td>1986.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>514.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2017-03-31 23:20:00</td>\n",
       "      <td>2017-03-31 23:30:00</td>\n",
       "      <td>3281.000000</td>\n",
       "      <td>3442.000000</td>\n",
       "      <td>29296.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>390893.000000</td>\n",
       "      <td>6515.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.563120</td>\n",
       "      <td>82.707121</td>\n",
       "      <td>989.974295</td>\n",
       "      <td>10.091335</td>\n",
       "      <td>10.091335</td>\n",
       "      <td>4011.550663</td>\n",
       "      <td>66.858684</td>\n",
       "      <td>0.822335</td>\n",
       "      <td>2.398100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Start Time                      Stop Time  \\\n",
       "count                          20400                          20400   \n",
       "mean   2017-02-21 20:03:51.773529344  2017-02-21 20:13:16.335293952   \n",
       "min              2017-01-01 00:38:00            2017-01-01 01:03:00   \n",
       "25%              2017-01-31 19:00:15            2017-01-31 19:04:45   \n",
       "50%              2017-02-27 07:20:00            2017-02-27 07:29:30   \n",
       "75%              2017-03-11 20:38:00            2017-03-11 20:49:00   \n",
       "max              2017-03-31 23:20:00            2017-03-31 23:30:00   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "       Start Station ID  End Station ID       Bike ID    Birth Year  \\\n",
       "count      20400.000000    20400.000000  20400.000000  20400.000000   \n",
       "mean        3215.863627     3211.439510  25301.732647   1979.319706   \n",
       "min         3183.000000      152.000000  15084.000000   1931.000000   \n",
       "25%         3186.000000     3186.000000  24523.000000   1974.000000   \n",
       "50%         3203.000000     3202.000000  24679.000000   1982.000000   \n",
       "75%         3267.000000     3220.000000  26220.000000   1986.000000   \n",
       "max         3281.000000     3442.000000  29296.000000   1999.000000   \n",
       "std           34.563120       82.707121    989.974295     10.091335   \n",
       "\n",
       "                Age  Trip Duration  Trip_Duration_in_min         Month  \\\n",
       "count  20400.000000   20400.000000          20400.000000  20400.000000   \n",
       "mean      41.680294     563.842745              9.398775      2.221569   \n",
       "min       22.000000      61.000000              1.000000      1.000000   \n",
       "25%       35.000000     221.000000              4.000000      1.000000   \n",
       "50%       39.000000     311.000000              5.000000      2.000000   \n",
       "75%       47.000000     514.000000              9.000000      3.000000   \n",
       "max       90.000000  390893.000000           6515.000000      3.000000   \n",
       "std       10.091335    4011.550663             66.858684      0.822335   \n",
       "\n",
       "        Temperature  \n",
       "count  20400.000000  \n",
       "mean      14.897647  \n",
       "min        9.000000  \n",
       "25%       13.000000  \n",
       "50%       15.000000  \n",
       "75%       17.000000  \n",
       "max       19.000000  \n",
       "std        2.398100  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display basic statistical information\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW9kMjT0vP6V"
   },
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "First, the dataset will be evaluated for inconsistencies or errors. This section will check for any missing values for each column. Then, an appropriate method of filling these values will be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "a3HvMQ3svTA6"
   },
   "outputs": [],
   "source": [
    "# Helper functions to check DataFrame\n",
    "def count_missing(df):\n",
    "    \"\"\"\n",
    "    Count number of missing values for each column in DataFrame `df`.\n",
    "    \"\"\"\n",
    "    return df.isnull().sum()\n",
    "\n",
    "def total_missing_pct(df):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of all missing cells.\n",
    "    \"\"\"\n",
    "    total_cells = np.prod(df.shape)\n",
    "    total_missing = count_missing(df).sum()\n",
    "\n",
    "    return (total_missing / total_cells) * 100\n",
    "\n",
    "def check_missing(df):\n",
    "    \"\"\"\n",
    "    Prints out description of missing values per column in a dataframe `df`.\n",
    "    \"\"\"\n",
    "    missing_count = count_missing(df)\n",
    "\n",
    "    print(missing_count)\n",
    "    print(f\"Total missing cells percentage: {total_missing_pct(df):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this instance, we will just remove rows with duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "cleaned_df = df.copy()\n",
    "\n",
    "cleaned_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to inspect each column for missing values. As such, the helper method `check_missing` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time              0\n",
      "Stop Time               0\n",
      "Start Station ID        0\n",
      "Start Station Name      0\n",
      "End Station ID          0\n",
      "End Station Name        1\n",
      "Bike ID                 0\n",
      "User Type               0\n",
      "Birth Year              0\n",
      "Age                     0\n",
      "Age Groups              0\n",
      "Trip Duration           0\n",
      "Trip_Duration_in_min    0\n",
      "Month                   0\n",
      "Season                  0\n",
      "Temperature             0\n",
      "Weekday                 0\n",
      "dtype: int64\n",
      "Total missing cells percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Check number of null values per column\n",
    "check_missing(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_cols(df):\n",
    "  \"\"\"\n",
    "  Retrieve columns from Dataframe `df` with missing values.\n",
    "  \"\"\"\n",
    "  COUNT_COL = \"Count\"\n",
    "  \n",
    "  missing_cols_df = count_missing(df).to_frame(name=COUNT_COL)\n",
    "  missing_cols = missing_cols_df[missing_cols_df[COUNT_COL] > 0] \\\n",
    "    .index \\\n",
    "    .tolist()\n",
    "    \n",
    "  return missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['End Station Name']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve columns with missing values\n",
    "missing_cols = get_missing_cols(cleaned_df)\n",
    "\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given than only the `End Station Name` columns has missing values, corresponding rows will be dropped, with a focus on the aforementioned column. Re-checking the missing values for columns should now be zero for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time              0\n",
      "Stop Time               0\n",
      "Start Station ID        0\n",
      "Start Station Name      0\n",
      "End Station ID          0\n",
      "End Station Name        0\n",
      "Bike ID                 0\n",
      "User Type               0\n",
      "Birth Year              0\n",
      "Age                     0\n",
      "Age Groups              0\n",
      "Trip Duration           0\n",
      "Trip_Duration_in_min    0\n",
      "Month                   0\n",
      "Season                  0\n",
      "Temperature             0\n",
      "Weekday                 0\n",
      "dtype: int64\n",
      "Total missing cells percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with blank columns\n",
    "cleaned_df.dropna(subset=missing_cols, inplace=True)\n",
    "\n",
    "# Double-check missing values\n",
    "check_missing(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "As mentioned in [Tutorial 3](https://careerfoundry.com/en/tutorials/data-analytics-for-beginners/descriptive-statistics-and-exploratory-data-analysis), descriptive statistics can be utilised to notice any outliers in our data. As such, we will calculate statistics such as `median`, `mean`, `min`, and `max` to our variables `Trip Duration` and `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc_stats(df, cols, aggregates=[\"mean\", \"median\", \"min\", \"max\"]):\n",
    "  \"\"\"\n",
    "  Creates a dataframe containing aggregated methods in `aggregates` for columns `cols`.\n",
    "\n",
    "  Aggregate methods default to `mean`, `median`, `min`, and `max`.\n",
    "  \"\"\"\n",
    "  return df[cols].agg(aggregates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip_Duration_in_min</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.398775</td>\n",
       "      <td>41.680294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6515.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Trip_Duration_in_min        Age\n",
       "mean                9.398775  41.680294\n",
       "median              5.000000  39.000000\n",
       "min                 1.000000  22.000000\n",
       "max              6515.000000  90.000000"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate statistics for columns\n",
    "desc_stats = get_desc_stats(df, [\"Trip_Duration_in_min\", \"Age\"])\n",
    "\n",
    "desc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the statistics above, we can see that the maximum value for the trip duration in minutes is abnormally high and quite unrealistic. As such we can safely remove records containing this value, and see if there are any more outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve max value from dataframe\n",
    "max_trip_duration_min = desc_stats.loc[\"max\"][\"Trip_Duration_in_min\"]\n",
    "# Indices of records with trip duration matching max value\n",
    "drop_indices = cleaned_df[cleaned_df[\"Trip_Duration_in_min\"] == max_trip_duration_min].index\n",
    "\n",
    "# Drop rows by indices\n",
    "cleaned_df.drop(drop_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip_Duration_in_min</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.192068</td>\n",
       "      <td>41.696016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3693.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Trip_Duration_in_min        Age\n",
       "mean                9.192068  41.696016\n",
       "median              5.000000  39.000000\n",
       "min                 1.000000  22.000000\n",
       "max              3693.000000  90.000000"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-check descriptive statistics after dropping outliers\n",
    "get_desc_stats(cleaned_df, [\"Trip_Duration_in_min\", \"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg3ib_N9VBrn"
   },
   "source": [
    "## Migration to SQL\n",
    "\n",
    "The dataset is now prepared to be formatted into their corresponding normalised tables to be imported to SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "1N__ES6NwzJn",
    "outputId": "06bc22dc-6d64-4e0f-d5d4-aa55463a7d53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Stop Time</th>\n",
       "      <th>Start Station ID</th>\n",
       "      <th>Start Station Name</th>\n",
       "      <th>End Station ID</th>\n",
       "      <th>End Station Name</th>\n",
       "      <th>Bike ID</th>\n",
       "      <th>User Type</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Groups</th>\n",
       "      <th>Trip Duration (Seconds)</th>\n",
       "      <th>Trip_Duration_in_min</th>\n",
       "      <th>Season</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:38:00</td>\n",
       "      <td>2017-01-01 01:03:00</td>\n",
       "      <td>3194</td>\n",
       "      <td>McGinley Square</td>\n",
       "      <td>3271</td>\n",
       "      <td>Danforth Light Rail</td>\n",
       "      <td>24668</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1961</td>\n",
       "      <td>60</td>\n",
       "      <td>55-64</td>\n",
       "      <td>1513</td>\n",
       "      <td>25</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 01:47:00</td>\n",
       "      <td>2017-01-01 01:58:00</td>\n",
       "      <td>3183</td>\n",
       "      <td>Exchange Place</td>\n",
       "      <td>3203</td>\n",
       "      <td>Hamilton Park</td>\n",
       "      <td>26167</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1993</td>\n",
       "      <td>28</td>\n",
       "      <td>25-34</td>\n",
       "      <td>639</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 01:56:00</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>3186</td>\n",
       "      <td>Grove St PATH</td>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>24604</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1970</td>\n",
       "      <td>51</td>\n",
       "      <td>45-54</td>\n",
       "      <td>258</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 02:12:00</td>\n",
       "      <td>2017-01-01 02:23:00</td>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>3206</td>\n",
       "      <td>Hilltop</td>\n",
       "      <td>24641</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1978</td>\n",
       "      <td>43</td>\n",
       "      <td>35-44</td>\n",
       "      <td>663</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-01 02:22:00</td>\n",
       "      <td>2017-01-01 02:31:00</td>\n",
       "      <td>3212</td>\n",
       "      <td>Christ Hospital</td>\n",
       "      <td>3225</td>\n",
       "      <td>Baldwin at Montgomery</td>\n",
       "      <td>24520</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1987</td>\n",
       "      <td>34</td>\n",
       "      <td>25-34</td>\n",
       "      <td>535</td>\n",
       "      <td>9</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Start Time           Stop Time  Start Station ID  \\\n",
       "0 2017-01-01 00:38:00 2017-01-01 01:03:00              3194   \n",
       "1 2017-01-01 01:47:00 2017-01-01 01:58:00              3183   \n",
       "3 2017-01-01 01:56:00 2017-01-01 02:00:00              3186   \n",
       "4 2017-01-01 02:12:00 2017-01-01 02:23:00              3270   \n",
       "5 2017-01-01 02:22:00 2017-01-01 02:31:00              3212   \n",
       "\n",
       "  Start Station Name  End Station ID       End Station Name  Bike ID  \\\n",
       "0    McGinley Square            3271    Danforth Light Rail    24668   \n",
       "1     Exchange Place            3203          Hamilton Park    26167   \n",
       "3      Grove St PATH            3270        Jersey & 6th St    24604   \n",
       "4    Jersey & 6th St            3206                Hilltop    24641   \n",
       "5    Christ Hospital            3225  Baldwin at Montgomery    24520   \n",
       "\n",
       "    User Type  Birth Year  Age Age Groups  Trip Duration (Seconds)  \\\n",
       "0  Subscriber        1961   60      55-64                     1513   \n",
       "1  Subscriber        1993   28      25-34                      639   \n",
       "3  Subscriber        1970   51      45-54                      258   \n",
       "4  Subscriber        1978   43      35-44                      663   \n",
       "5  Subscriber        1987   34      25-34                      535   \n",
       "\n",
       "   Trip_Duration_in_min  Season  Temperature  \n",
       "0                    25  Winter           10  \n",
       "1                    11  Winter           10  \n",
       "3                     4  Winter           10  \n",
       "4                    11  Winter           10  \n",
       "5                     9  Winter           10  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns for corresponding tables are extracted\n",
    "cols = cleaned_df.columns.tolist()\n",
    "\n",
    "# Remove columns that can be calculated in demand (e.g. Month, Duration in Minutes)\n",
    "cleaned_df.drop(columns=[\"Month\", \"Weekday\"], inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "cleaned_df.rename(columns={\"Trip Duration\": \"Trip Duration (Seconds)\"}, inplace=True)\n",
    "\n",
    "station_start_columns = [col for col in cols if \"Start Station\" in col]\n",
    "station_end_columns = [col for col in cols if \"End Station\" in col]\n",
    "user_columns = cols[6:11]\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, dataframes will be created separately for start and end stations. This is essential for easier extraction, as well as for removal of its duplicates at a later stage when merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "n1_xdyxPwz_8"
   },
   "outputs": [],
   "source": [
    "# Dataframes for start and end locations\n",
    "# Duplicates are then removed based on their station IDs\n",
    "station_start_df = cleaned_df[station_start_columns].drop_duplicates(subset=[station_start_columns[0]])\n",
    "station_end_df = cleaned_df[station_end_columns].drop_duplicates(subset=[station_end_columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "fgJ-o21Mpc6e",
    "outputId": "b4a11946-08b0-43a3-ad6c-8e0e1123b9ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3194</td>\n",
       "      <td>McGinley Square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3183</td>\n",
       "      <td>Exchange Place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3186</td>\n",
       "      <td>Grove St PATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3212</td>\n",
       "      <td>Christ Hospital</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID             Name\n",
       "0  3194  McGinley Square\n",
       "1  3183   Exchange Place\n",
       "2  3186    Grove St PATH\n",
       "3  3270  Jersey & 6th St\n",
       "4  3212  Christ Hospital"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns for both dataframes to allow concatenation\n",
    "station_start_df.rename(columns=lambda c: c.replace(\"Start Station \", \"\"),\n",
    "                        inplace=True)\n",
    "station_end_df.rename(columns=lambda c: c.replace(\"End Station \", \"\"),\n",
    "                      inplace=True)\n",
    "\n",
    "# Merge the start and end dataframes\n",
    "station_df = pd \\\n",
    "  .concat([station_start_df, station_end_df], ignore_index=True) \\\n",
    "  .drop_duplicates()\n",
    "\n",
    "# List all stations\n",
    "station_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are some inconsistencies with regards to the station name. For example, `Grove St PATH` has the word `path` in uppercase. We can fix this by doing a simple `map()` operation to our `station_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Input text\n",
    "text = \"St John lives on St Patrick street. The statue is near St. Michael's Church.\"\n",
    "\n",
    "def clean_station_name(name):\n",
    "  \"\"\"\n",
    "  Perform cleaning of the provided station name for a more consistent naming convention.\n",
    "  \"\"\"\n",
    "  # Regex pattern to match \"St\" not followed by a dot or within a word\n",
    "  result = re.sub(r'\\bSt\\b(?!\\.)', 'St.', name)\n",
    "  \n",
    "  # Replace \"PATH\" with \"Path\"\n",
    "  result = result.title() if \"PATH\" in result else result\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3194</td>\n",
       "      <td>McGinley Square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3183</td>\n",
       "      <td>Exchange Place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3186</td>\n",
       "      <td>Grove St. Path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3212</td>\n",
       "      <td>Christ Hospital</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID              Name\n",
       "0  3194   McGinley Square\n",
       "1  3183    Exchange Place\n",
       "2  3186    Grove St. Path\n",
       "3  3270  Jersey & 6th St.\n",
       "4  3212   Christ Hospital"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_df[\"Name\"] = station_df[\"Name\"].apply(clean_station_name)\n",
    "\n",
    "station_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, a dataframe for `User` will be created, checking for duplicates before any actual processing is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(df, subset):\n",
    "  \"\"\"\n",
    "  Checks dataframe `df` for any duplicates based on provided `subset`, else defaults to all features.\n",
    "  \"\"\"\n",
    "  result = df.duplicated(subset=subset).sum()\n",
    "  \n",
    "  return result > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "tj4rgli0F3cK",
    "outputId": "474a27d9-079c-477c-bbdc-7b887c40e177"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Bike ID</th>\n",
       "      <th>User Type</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Groups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24668</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1961</td>\n",
       "      <td>60</td>\n",
       "      <td>55-64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26167</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1993</td>\n",
       "      <td>28</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>24604</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1970</td>\n",
       "      <td>51</td>\n",
       "      <td>45-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24641</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1978</td>\n",
       "      <td>43</td>\n",
       "      <td>35-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>24520</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1987</td>\n",
       "      <td>34</td>\n",
       "      <td>25-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Bike ID   User Type  Birth Year  Age Age Groups\n",
       "0   0    24668  Subscriber        1961   60      55-64\n",
       "1   1    26167  Subscriber        1993   28      25-34\n",
       "2   3    24604  Subscriber        1970   51      45-54\n",
       "3   4    24641  Subscriber        1978   43      35-44\n",
       "4   5    24520  Subscriber        1987   34      25-34"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_ID = \"ID\"\n",
    "\n",
    "# Dataframe for user data\n",
    "user_df = cleaned_df[user_columns].reset_index(names=DEFAULT_ID)\n",
    "\n",
    "# Check the dataframe for duplicates; drop if there are any\n",
    "if has_duplicates(user_df, DEFAULT_ID):\n",
    "  user_df.drop_duplicates()\n",
    "\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As extra precaution, we need to ensure that a user record corresponds to a single trip, hence a 1:1 match. To do this, the number of rows for each dataframe will be compared. This is to see if a compound unique key needs to be created for the two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compound unique keey needed.\n"
     ]
    }
   ],
   "source": [
    "if user_df.shape[0] == cleaned_df.shape[0]:\n",
    "  print(\"No compound unique keey needed.\")\n",
    "else:\n",
    "  print(\"Creating compound unique key...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are certain that each `User` record is unique and has a corresponding `Trip` record, and have the means of defining its relationship by way of `ID` column.\n",
    "\n",
    "In relation to the steps performed for `user_df`, the same needs to be applied to our base dataframe to ensure that `ID` column for `User` can be properly set as foreign key to our `Trip` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Stop Time</th>\n",
       "      <th>Start Station ID</th>\n",
       "      <th>Start Station Name</th>\n",
       "      <th>End Station ID</th>\n",
       "      <th>End Station Name</th>\n",
       "      <th>Bike ID</th>\n",
       "      <th>User Type</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Groups</th>\n",
       "      <th>Trip Duration (Seconds)</th>\n",
       "      <th>Trip_Duration_in_min</th>\n",
       "      <th>Season</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:38:00</td>\n",
       "      <td>2017-01-01 01:03:00</td>\n",
       "      <td>3194</td>\n",
       "      <td>McGinley Square</td>\n",
       "      <td>3271</td>\n",
       "      <td>Danforth Light Rail</td>\n",
       "      <td>24668</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1961</td>\n",
       "      <td>60</td>\n",
       "      <td>55-64</td>\n",
       "      <td>1513</td>\n",
       "      <td>25</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 01:47:00</td>\n",
       "      <td>2017-01-01 01:58:00</td>\n",
       "      <td>3183</td>\n",
       "      <td>Exchange Place</td>\n",
       "      <td>3203</td>\n",
       "      <td>Hamilton Park</td>\n",
       "      <td>26167</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1993</td>\n",
       "      <td>28</td>\n",
       "      <td>25-34</td>\n",
       "      <td>639</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-01-01 01:56:00</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>3186</td>\n",
       "      <td>Grove St PATH</td>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>24604</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1970</td>\n",
       "      <td>51</td>\n",
       "      <td>45-54</td>\n",
       "      <td>258</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01 02:12:00</td>\n",
       "      <td>2017-01-01 02:23:00</td>\n",
       "      <td>3270</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>3206</td>\n",
       "      <td>Hilltop</td>\n",
       "      <td>24641</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1978</td>\n",
       "      <td>43</td>\n",
       "      <td>35-44</td>\n",
       "      <td>663</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-01 02:22:00</td>\n",
       "      <td>2017-01-01 02:31:00</td>\n",
       "      <td>3212</td>\n",
       "      <td>Christ Hospital</td>\n",
       "      <td>3225</td>\n",
       "      <td>Baldwin at Montgomery</td>\n",
       "      <td>24520</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1987</td>\n",
       "      <td>34</td>\n",
       "      <td>25-34</td>\n",
       "      <td>535</td>\n",
       "      <td>9</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID          Start Time           Stop Time  Start Station ID  \\\n",
       "0   0 2017-01-01 00:38:00 2017-01-01 01:03:00              3194   \n",
       "1   1 2017-01-01 01:47:00 2017-01-01 01:58:00              3183   \n",
       "2   3 2017-01-01 01:56:00 2017-01-01 02:00:00              3186   \n",
       "3   4 2017-01-01 02:12:00 2017-01-01 02:23:00              3270   \n",
       "4   5 2017-01-01 02:22:00 2017-01-01 02:31:00              3212   \n",
       "\n",
       "  Start Station Name  End Station ID       End Station Name  Bike ID  \\\n",
       "0    McGinley Square            3271    Danforth Light Rail    24668   \n",
       "1     Exchange Place            3203          Hamilton Park    26167   \n",
       "2      Grove St PATH            3270        Jersey & 6th St    24604   \n",
       "3    Jersey & 6th St            3206                Hilltop    24641   \n",
       "4    Christ Hospital            3225  Baldwin at Montgomery    24520   \n",
       "\n",
       "    User Type  Birth Year  Age Age Groups  Trip Duration (Seconds)  \\\n",
       "0  Subscriber        1961   60      55-64                     1513   \n",
       "1  Subscriber        1993   28      25-34                      639   \n",
       "2  Subscriber        1970   51      45-54                      258   \n",
       "3  Subscriber        1978   43      35-44                      663   \n",
       "4  Subscriber        1987   34      25-34                      535   \n",
       "\n",
       "   Trip_Duration_in_min  Season  Temperature  \n",
       "0                    25  Winter           10  \n",
       "1                    11  Winter           10  \n",
       "2                     4  Winter           10  \n",
       "3                    11  Winter           10  \n",
       "4                     9  Winter           10  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expose the `index` column\n",
    "cleaned_df.reset_index(names=DEFAULT_ID, inplace=True)\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "Un6Pyrc2JjYd",
    "outputId": "d730ba76-ae21-48fb-eb02-0d601033c3df"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Start Station Name', 'End Station Name', 'Trip_Duration_in_min', 'Bike ID', 'User Type', 'Birth Year', 'Age', 'Age Groups'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Remove unnecessary columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcleaned_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStart Station Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnd Station Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrip_Duration_in_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muser_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Rename station columns for a more general one\u001b[39;00m\n\u001b[1;32m      5\u001b[0m cleaned_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Station ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Station\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd Station ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd Station\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m                   inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Projects/Data Analysis/nyc-careerfoundry/.venv/lib/python3.9/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/Data Analysis/nyc-careerfoundry/.venv/lib/python3.9/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Documents/Projects/Data Analysis/nyc-careerfoundry/.venv/lib/python3.9/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/Data Analysis/nyc-careerfoundry/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Start Station Name', 'End Station Name', 'Trip_Duration_in_min', 'Bike ID', 'User Type', 'Birth Year', 'Age', 'Age Groups'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "cleaned_df.drop(columns=[\"Start Station Name\", \"End Station Name\", \"Trip_Duration_in_min\"] + user_columns,\n",
    "                inplace=True)\n",
    "# Rename station columns for a more general one\n",
    "cleaned_df.rename(columns={\"Start Station ID\": \"Start Station\",\n",
    "                           \"End Station ID\": \"End Station\"},\n",
    "                  inplace=True)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPWJPxFeqIZM"
   },
   "source": [
    "## MySQL\n",
    "\n",
    "In this stage, the data is now ready to be migrated to our SQL database. The necessary connections need to be established for a successful migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "xZvLwUdMqK_1"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "\n",
    "\n",
    "def create_db_engine(user, pw, db=None):\n",
    "    \"\"\"\n",
    "    Create and engine to connect to MySQL databases.\n",
    "    \"\"\"\n",
    "    engine = create_engine(f\"mysql+pymysql://{user}:{pw}@localhost/{db}\")\n",
    "    \n",
    "    if not database_exists(engine.url):\n",
    "        create_database(engine.url)\n",
    "    \n",
    "    return engine\n",
    "\n",
    "def to_snake_case(s):\n",
    "    \"\"\"\n",
    "    Convert a string `s` to snake case.\n",
    "    \"\"\"\n",
    "    split = s.lower().split(\" \")\n",
    "    joined = \"_\".join(split)\n",
    "    cleaned = re.sub(r'\\W+', '', joined)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def insert_to_db(df, table_name, engine):\n",
    "    \"\"\"\n",
    "    Insert dataframe to SQL database using provided engine. In turn,\n",
    "    the primary key will be added, using the first dataframe column by default.\n",
    "    \n",
    "    Additionaly, converts column names to snake case for easier usage within SQL.\n",
    "    \"\"\"\n",
    "    # Create new dataframe with columns formatted to snake case\n",
    "    to_insert_df = df.rename(columns=lambda c: to_snake_case(c))\n",
    "    # Insert to SQL using engine\n",
    "    to_insert_df.to_sql(table_name, \n",
    "                        engine, \n",
    "                        index=False, \n",
    "                        index_label=DEFAULT_ID.lower(),\n",
    "                        if_exists=\"replace\")\n",
    "    # Add primary key\n",
    "    pk = to_insert_df.columns.tolist()[0]\n",
    "    \n",
    "    alter_db(engine.connect(), f\"ALTER TABLE {table_name} ADD PRIMARY KEY (`{pk}`);\")\n",
    "    \n",
    "def alter_db(connection, statement):\n",
    "    \"\"\"\n",
    "    Perform altering operations to a database in the given connection.\n",
    "    \"\"\"\n",
    "    trans = connection.begin()\n",
    "    \n",
    "    try:\n",
    "        connection.execute(text(statement))\n",
    "        trans.commit()\n",
    "    except Exception:\n",
    "        trans.rollback()\n",
    "        \n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_db_credentials():\n",
    "  \"\"\"\n",
    "  Read credentials specified in a local .env file to be used for establishing database connection.\n",
    "  \"\"\"\n",
    "\n",
    "  DB_USER = os.getenv(\"DB_USER\")\n",
    "  DB_PW = os.getenv(\"DB_PW\")\n",
    "  \n",
    "  return (DB_USER, DB_PW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "DMHqGfcKvKPo",
    "outputId": "ff8da32d-dbf8-41b2-bd54-3518d9d99b53"
   },
   "outputs": [],
   "source": [
    "DB_NAME = \"nyc\"\n",
    "\n",
    "# Collect user credentials from local .env for a more secure connection\n",
    "user, password = get_env_db_credentials()\n",
    "engine = create_db_engine(user, password, DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert dataframe to SQL table `Trip`\n",
    "insert_to_db(cleaned_df, \"Trip\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert Station dataframe to SQL table\n",
    "insert_to_db(station_df, \"Station\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert User dataframe to SQL table\n",
    "insert_to_db(user_df, \"User\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = engine.connect()\n",
    "\n",
    "# Add necessary foreign keys\n",
    "alter_db(connection,\n",
    "  f\"\"\"\n",
    "  ALTER TABLE Trip \n",
    "  ADD CONSTRAINT FK_StartTrip FOREIGN KEY (start_station) REFERENCES Station({DEFAULT_ID.lower()}),\n",
    "  ADD CONSTRAINT FK_EndTrip FOREIGN KEY (end_station) REFERENCES Station({DEFAULT_ID.lower()}),\n",
    "  ADD CONSTRAINT FK_UserTrip FOREIGN KEY ({DEFAULT_ID.lower()}) REFERENCES User({DEFAULT_ID.lower()});\n",
    "  \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
